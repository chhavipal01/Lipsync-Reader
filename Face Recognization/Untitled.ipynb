{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6409aa9-4019-4bd7-b9ec-52cfc4dca1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "print(\"OpenCV version:\", cv2.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8f8b2d-b088-4f78-a637-e85add070b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "print(\"OpenCV version:\", cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943941be-9865-4d5a-9f58-cb0e9fe5d02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import cv2\n",
    "\n",
    "# Step 1: Download the LFW dataset\n",
    "lfw_url = 'http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.zip'\n",
    "dataset_dir = 'lfw_dataset'\n",
    "zip_path = 'lfw-deepfunneled.zip'\n",
    "\n",
    "if not os.path.exists(zip_path):\n",
    "    print(\"üì¶ Downloading LFW dataset...\")\n",
    "    urllib.request.urlretrieve(lfw_url, zip_path)\n",
    "    print(\"‚úÖ Download complete!\")\n",
    "\n",
    "# Step 2: Unzip the dataset\n",
    "if not os.path.exists(dataset_dir):\n",
    "    print(\"üìÇ Extracting zip file...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dataset_dir)\n",
    "    print(\"‚úÖ Extraction complete!\")\n",
    "\n",
    "# Step 3: Load and process images\n",
    "processed_dir = 'processed_faces'\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "print(\"üß† Processing images...\")\n",
    "\n",
    "image_count = 0\n",
    "for root, dirs, files in os.walk(os.path.join(dataset_dir, \"lfw-deepfunneled\")):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jpg\"):\n",
    "            image_path = os.path.join(root, file)\n",
    "            img = cv2.imread(image_path)\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            resized = cv2.resize(gray, (100, 100))  # Resize for consistency\n",
    "\n",
    "            # Optional: Save processed image\n",
    "            save_path = os.path.join(processed_dir, f\"face_{image_count}.jpg\")\n",
    "            cv2.imwrite(save_path, resized)\n",
    "            image_count += 1\n",
    "\n",
    "print(f\"‚úÖ Processed and saved {image_count} face images to '{processed_dir}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74559c97-e318-47a9-b099-64567d386eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Step 1: Download dataset (only faces with at least 70 images)\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.5, color=False)\n",
    "\n",
    "print(\"‚úÖ Dataset loaded!\")\n",
    "print(\"Total images:\", len(lfw_people.images))\n",
    "print(\"Image shape:\", lfw_people.images[0].shape)\n",
    "\n",
    "# Step 2: Save as images (optional)\n",
    "output_dir = \"processed_lfw_faces\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for i, image in enumerate(lfw_people.images):\n",
    "    filename = os.path.join(output_dir, f\"face_{i}.jpg\")\n",
    "    cv2.imwrite(filename, image)\n",
    "\n",
    "print(f\"‚úÖ Saved {len(lfw_people.images)} grayscale face images in '{output_dir}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36823970-9223-4627-b20f-668300bf13f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Start video capture (0 = default webcam)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if the webcam opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"‚ùå Cannot access webcam\")\n",
    "else:\n",
    "    print(\"üé• Webcam access granted. Press 'q' to quit.\")\n",
    "\n",
    "# Read and display video frames\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"‚ùå Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    cv2.imshow('Webcam Feed', frame)\n",
    "\n",
    "    # Press 'q' to exit the webcam view\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release and cleanup\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6d7566-a51c-427b-b596-c3ace8b51841",
   "metadata": {},
   "outputs": [],
   "source": [
    "q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc22e684-8cde-4605-a339-2b6a898327af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Start capturing\n",
    "cap = cv2.VideoCapture(0)  # 0 = default webcam\n",
    "\n",
    "# Loop to keep webcam on\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    cv2.imshow('Webcam - Press Q to quit', frame)\n",
    "\n",
    "    # Wait for 'q' key to stop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0913e623-d62b-4095-8f5c-c77bc4d8223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Start capturing\n",
    "cap = cv2.VideoCapture(0)  # 0 = default webcam\n",
    "\n",
    "# Loop to keep webcam on\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    cv2.imshow('Webcam - Press Q to quit', frame)\n",
    "\n",
    "    # Wait for 'q' key to stop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ee4d2f-bc52-4529-bcbc-53b2f5261f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "import numpy as np\n",
    "\n",
    "# Load grayscale LFW dataset\n",
    "lfw_people = fetch_lfw_people(color=False, resize=0.5)\n",
    "X = lfw_people.images\n",
    "y = lfw_people.target\n",
    "target_names = lfw_people.target_names\n",
    "\n",
    "# Normalize pixel values (0 to 1)\n",
    "X = X / 255.0\n",
    "\n",
    "# Reshape for TensorFlow (samples, height, width, channels)\n",
    "X = X.reshape(-1, X.shape[1], X.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039435df-06c1-4464-a2ba-efdabfef328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=X.shape[1:]),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(target_names), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463dd158-9cfc-46f4-8609-231420ec1874",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a95c415-4c1f-4ce0-9ef7-cf902923a36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Test accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18438528-bb88-41c3-b481-c6803f63218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction on one image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "index = 0\n",
    "sample_image = X_test[index]\n",
    "sample_label = y_test[index]\n",
    "\n",
    "# Add batch dimension\n",
    "prediction = model.predict(sample_image.reshape(1, 62, 47, 1))\n",
    "predicted_label = target_names[np.argmax(prediction)]\n",
    "\n",
    "# Show image\n",
    "plt.imshow(sample_image.reshape(62, 47), cmap='gray')\n",
    "plt.title(f\"Predicted: {predicted_label}\\nActual: {target_names[sample_label]}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e68289-07b6-4c52-8e30-162ad1e6b1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "import numpy as np\n",
    "\n",
    "# Load grayscale LFW dataset\n",
    "lfw_people = fetch_lfw_people(color=False, resize=0.5)\n",
    "X = lfw_people.images\n",
    "y = lfw_people.target\n",
    "target_names = lfw_people.target_names\n",
    "\n",
    "# Normalize pixel values (0 to 1)\n",
    "X = X / 255.0\n",
    "\n",
    "# Reshape for TensorFlow (samples, height, width, channels)\n",
    "X = X.reshape(-1, X.shape[1], X.shape[2], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a9b189-20e5-46b8-bd13-71800f0eabce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Load grayscale LFW dataset\n",
    "lfw_people = fetch_lfw_people(color=False, resize=0.5)\n",
    "X = lfw_people.images\n",
    "y = lfw_people.target\n",
    "target_names = lfw_people.target_names\n",
    "\n",
    "# Normalize and reshape\n",
    "X = X / 255.0\n",
    "X = X.reshape(-1, X.shape[1], X.shape[2], 1)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=X.shape[1:]),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(target_names), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile and train\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"‚úÖ Test accuracy:\", test_acc)\n",
    "\n",
    "# Predict on one test image\n",
    "index = 0\n",
    "sample_image = X_test[index]\n",
    "sample_label = y_test[index]\n",
    "prediction = model.predict(sample_image.reshape(1, 62, 47, 1))\n",
    "predicted_label = target_names[np.argmax(prediction)]\n",
    "\n",
    "# Show image with prediction\n",
    "plt.imshow(sample_image.reshape(62, 47), cmap='gray')\n",
    "plt.title(f\"Predicted: {predicted_label}\\nActual: {target_names[sample_label]}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f61368-1d5f-4f22-a1f7-e7c2e1a9a1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Load grayscale LFW dataset\n",
    "lfw_people = fetch_lfw_people(color=False, resize=0.5)\n",
    "X = lfw_people.images\n",
    "y = lfw_people.target\n",
    "target_names = lfw_people.target_names\n",
    "\n",
    "# Normalize and reshape\n",
    "X = X / 255.0\n",
    "X = X.reshape(-1, X.shape[1], X.shape[2], 1)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=X.shape[1:]),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(target_names), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile and train\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"‚úÖ Test accuracy:\", test_acc)\n",
    "\n",
    "# Predict on one test image\n",
    "index = 0\n",
    "sample_image = X_test[index]\n",
    "sample_label = y_test[index]\n",
    "prediction = model.predict(sample_image.reshape(1, 62, 47, 1))\n",
    "predicted_label = target_names[np.argmax(prediction)]\n",
    "\n",
    "# Show image with prediction\n",
    "plt.imshow(sample_image.reshape(62, 47), cmap='gray')\n",
    "plt.title(f\"Predicted: {predicted_label}\\nActual: {target_names[sample_label]}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8193f92-1ab3-4f95-a932-a870dc5d2b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Set the path to the extracted dataset folder\n",
    "dataset_path = \"LFW-Beautified-dataset\"\n",
    "\n",
    "# Target size for resizing\n",
    "img_size = (62, 47)  # width x height\n",
    "X = []\n",
    "y = []\n",
    "labels = {}\n",
    "\n",
    "label_count = 0\n",
    "\n",
    "# Walk through each folder (person)\n",
    "for person_name in os.listdir(dataset_path):\n",
    "    person_folder = os.path.join(dataset_path, person_name)\n",
    "    \n",
    "    if os.path.isdir(person_folder):\n",
    "        if person_name not in labels:\n",
    "            labels[person_name] = label_count\n",
    "            label_count += 1\n",
    "        \n",
    "        # Loop through images inside the person's folder\n",
    "        for img_name in os.listdir(person_folder):\n",
    "            img_path = os.path.join(person_folder, img_name)\n",
    "            try:\n",
    "                # Load, convert to grayscale, and resize\n",
    "                img = cv2.imread(img_path)\n",
    "                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                resized = cv2.resize(gray, img_size)\n",
    "\n",
    "                X.append(resized)\n",
    "                y.append(labels[person_name])\n",
    "            except Exception as e:\n",
    "                print(f\"Failed on {img_path}: {e}\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Normalize and reshape for TensorFlow\n",
    "X = X / 255.0\n",
    "X = X.reshape(-1, img_size[1], img_size[0], 1)  # (samples, height, width, channels)\n",
    "\n",
    "# Optional: Convert labels to one-hot encoding (if using categorical output)\n",
    "# y = to_categorical(y)\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"‚úÖ Dataset loaded and preprocessed!\")\n",
    "print(\"Total images:\", len(X))\n",
    "print(\"Image shape:\", X[0].shape)\n",
    "print(\"Total classes:\", len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad036b1-65d2-419a-a50b-b3ef22ea508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_name in os.listdir(person_folder):\n",
    "    img_path = os.path.join(person_folder, img_name)\n",
    "    \n",
    "    # Only process image files\n",
    "    if not img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "        continue\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"Skipped empty or corrupt: {img_path}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        resized = cv2.resize(gray, img_size)\n",
    "        X.append(resized)\n",
    "        y.append(labels[person_name])\n",
    "    except Exception as e:\n",
    "        print(f\"Failed on {img_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca02fc7-b543-4026-ad4a-a2c697ac20a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dataset_path = \"LFW-Beautified-dataset/LFW-Beautified-dataset\"\n",
    "\n",
    "for person_name in os.listdir(main_dataset_path):\n",
    "    person_folder = os.path.join(main_dataset_path, person_name)\n",
    "\n",
    "    # Skip if it's not a folder\n",
    "    if not os.path.isdir(person_folder):\n",
    "        print(f\"Skipping non-folder: {person_folder}\")\n",
    "        continue\n",
    "\n",
    "    # [Now loop over images inside person_folder...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c86b475-1fff-4fc8-b37a-f48c51dac9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Path to the extracted dataset folder\n",
    "main_dataset_path = \"LFW-Beautified-dataset/LFW-Beautified-dataset\"\n",
    "img_size = (100, 100)  # Resize target (you can change this)\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for person_name in os.listdir(main_dataset_path):\n",
    "    person_folder = os.path.join(main_dataset_path, person_name)\n",
    "\n",
    "    # Skip non-folder entries (e.g. zip, system files)\n",
    "    if not os.path.isdir(person_folder):\n",
    "        print(f\"Skipping non-folder: {person_folder}\")\n",
    "        continue\n",
    "\n",
    "    for img_name in os.listdir(person_folder):\n",
    "        img_path = os.path.join(person_folder, img_name)\n",
    "\n",
    "        # Optional: Only process image files\n",
    "        if not img_name.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if img is None:\n",
    "                raise ValueError(\"Image not loaded\")\n",
    "\n",
    "            img = cv2.resize(img, img_size)\n",
    "            X.append(img)\n",
    "            y.append(person_name)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed on {img_path}: {e}\")\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "X = X.reshape(-1, img_size[1], img_size[0], 1)  # Add channel dimension\n",
    "\n",
    "print(\"‚úÖ Dataset loaded and preprocessed!\")\n",
    "print(f\"Total images: {len(X)}\")\n",
    "print(f\"Unique labels: {len(np.unique(y))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b88f6df-78fd-42be-a59a-94a90ee1a0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Label encode the classes\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_categorical = to_categorical(y_encoded)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=X.shape[1:]),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(y_categorical.shape[1], activation='softmax')  # Output layer matches number of classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"‚úÖ Test accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6950354d-c4ad-4c9c-ac83-24aab27801dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1  # <-- Ensures training logs are printed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8662b8-d551-44c0-8c97-d25efff9c40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# One-hot encode labels if needed (uncomment if your labels are integers)\n",
    "# y_train = to_categorical(y_train)\n",
    "# y_test = to_categorical(y_test)\n",
    "\n",
    "# Build model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], 1)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(len(set(y)), activation='softmax')  # number of classes\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model with output\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"\\n‚úÖ Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Plot accuracy\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training vs Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d119ab3-5390-4371-adfc-7a8b8b429e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as display\n",
    "display.display(display.Markdown(f\"**‚úÖ Test Accuracy: `{test_acc:.4f}`**\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40106765-1a61-4885-b680-54a548d5217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Epochs:\", len(history.history['accuracy']))\n",
    "print(\"Final Training Accuracy:\", history.history['accuracy'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e93ab7-0266-4e72-845b-f9e26170bf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pick some test images\n",
    "num_samples = 5\n",
    "indices = np.random.choice(len(X_test), num_samples, replace=False)\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "for i, idx in enumerate(indices):\n",
    "    img = X_test[idx]\n",
    "    true_label = y_test[idx]\n",
    "\n",
    "    # Predict\n",
    "    prediction = model.predict(np.expand_dims(img, axis=0), verbose=0)\n",
    "    predicted_label = np.argmax(prediction)\n",
    "\n",
    "    # Plot image\n",
    "    plt.subplot(1, num_samples, i + 1)\n",
    "    plt.imshow(img.squeeze(), cmap='gray')\n",
    "    plt.title(f\"True: {true_label}\\nPred: {predicted_label}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.suptitle(\"üß† Model Predictions on Sample Test Images\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7317e164-6434-4bc9-a53e-b22e20af58b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6768d720-8889-4176-8627-75bffd2efcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample check\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d84c2b-f54f-41bf-adb4-ac93ea8fac2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Set dataset path\n",
    "main_dataset_path = \"LFW-Beautified-dataset/LFW-Beautified-dataset\"\n",
    "\n",
    "# Preprocessing config\n",
    "img_size = (100, 100)  # You can adjust this\n",
    "X = []\n",
    "y = []\n",
    "label_map = {}\n",
    "\n",
    "label_index = 0\n",
    "for person_name in os.listdir(main_dataset_path):\n",
    "    person_folder = os.path.join(main_dataset_path, person_name)\n",
    "    if not os.path.isdir(person_folder):\n",
    "        continue\n",
    "    if person_name not in label_map:\n",
    "        label_map[person_name] = label_index\n",
    "        label_index += 1\n",
    "\n",
    "    for img_name in os.listdir(person_folder):\n",
    "        img_path = os.path.join(person_folder, img_name)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            continue\n",
    "        img = cv2.resize(img, img_size)\n",
    "        X.append(img)\n",
    "        y.append(label_map[person_name])\n",
    "\n",
    "# Convert to NumPy and reshape\n",
    "X = np.array(X).reshape(-1, img_size[0], img_size[1], 1) / 255.0\n",
    "y = np.array(y)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_size[0], img_size[1], 1)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(len(label_map), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"‚úÖ Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Plot training vs validation accuracy\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.title('Training vs Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4137123-cc3b-4d28-a73f-477ab2ab376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "# Load and preprocess the image\n",
    "test_img_path = 'path_to_your_image.jpg'  # <-- change this\n",
    "img = cv2.imread(test_img_path, cv2.IMREAD_GRAYSCALE)\n",
    "img = cv2.resize(img, (img_size[0], img_size[1]))\n",
    "img = img_to_array(img) / 255.0\n",
    "img = np.expand_dims(img, axis=0)\n",
    "\n",
    "# Predict\n",
    "prediction = model.predict(img)\n",
    "predicted_class = np.argmax(prediction)\n",
    "\n",
    "print(\"Predicted class index:\", predicted_class)\n",
    "print(\"Prediction confidence:\", np.max(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5e41ee-7f65-4dbb-97ea-cbb8cf5151ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model as an HDF5 file\n",
    "model.save(\"face_lip_model.h5\")\n",
    "print(\"‚úÖ Model saved as face_lip_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb920128-20aa-4dfc-b98b-61227f18739e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the model\n",
    "model = load_model(\"face_lip_model.h5\")\n",
    "\n",
    "# Define your image size (same as used during training)\n",
    "img_size = (100, 100)  # update if you used a different size\n",
    "\n",
    "# Path to test image\n",
    "test_img_path = 'path_to_your_test_image.jpg'  # üîÅ Replace with your actual image path\n",
    "\n",
    "# Load and preprocess the image\n",
    "img = cv2.imread(test_img_path, cv2.IMREAD_GRAYSCALE)\n",
    "if img is None:\n",
    "    print(\"‚ùå Failed to load image. Check the path.\")\n",
    "else:\n",
    "    img = cv2.resize(img, img_size)\n",
    "    img = img_to_array(img) / 255.0\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "\n",
    "    # Predict\n",
    "    prediction = model.predict(img)\n",
    "    predicted_class = np.argmax(prediction)\n",
    "\n",
    "    # Show results\n",
    "    plt.imshow(cv2.imread(test_img_path))\n",
    "    plt.title(f\"Predicted Class: {predicted_class}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4484ef-adad-4c4c-86c0-23806ae31afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "\n",
    "# Define the path where the images are stored\n",
    "data_dir = 'LFWPeople'  # <- change this if needed\n",
    "img_size = (100, 100)   # <- use the same size you trained the model with\n",
    "\n",
    "# Choose a random subfolder (person)\n",
    "people = os.listdir(data_dir)\n",
    "random_person = random.choice(people)\n",
    "person_path = os.path.join(data_dir, random_person)\n",
    "\n",
    "# Choose a random image from that person\n",
    "image_name = random.choice(os.listdir(person_path))\n",
    "image_path = os.path.join(person_path, image_name)\n",
    "\n",
    "# Load and preprocess image\n",
    "img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "if img is None:\n",
    "    raise ValueError(\"‚ùå Couldn't read image. Please check path:\", image_path)\n",
    "\n",
    "img_resized = cv2.resize(img, img_size)\n",
    "img_array = img_to_array(img_resized) / 255.0\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "# Predict using the model\n",
    "prediction = model.predict(img_array)\n",
    "predicted_label = np.argmax(prediction)\n",
    "\n",
    "# Show image and prediction\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title(f\"Predicted label: {predicted_label}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c8094a-4e58-4db5-9538-67954aa8e3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "\n",
    "# Set the correct dataset path\n",
    "data_dir = r'C:\\Users\\user\\Desktop\\Face Recognization\\lfw_funneled'\n",
    "img_size = (100, 100)   # same as training size\n",
    "\n",
    "# Choose a random subfolder (person)\n",
    "people = os.listdir(data_dir)\n",
    "random_person = random.choice(people)\n",
    "person_path = os.path.join(data_dir, random_person)\n",
    "\n",
    "# Choose a random image from that person's folder\n",
    "img_name = random.choice(os.listdir(person_path))\n",
    "img_path = os.path.join(person_path, img_name)\n",
    "\n",
    "# Load and preprocess the image\n",
    "img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "img_resized = cv2.resize(img, img_size)\n",
    "img_array = img_to_array(img_resized) / 255.0\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "# Make a prediction\n",
    "prediction = model.predict(img_array)\n",
    "predicted_class = np.argmax(prediction)\n",
    "\n",
    "# Show result\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title(f'Predicted Class: {predicted_class}')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2675f0d4-77c5-4aff-8ae4-1ed319a582e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "\n",
    "# Set the dataset path and image size\n",
    "data_dir = r'C:\\Users\\user\\Desktop\\Face Recognization\\lfw_funneled'\n",
    "img_size = (100, 100)   # must match training\n",
    "\n",
    "# Build label map (folder names as labels)\n",
    "label_names = sorted(os.listdir(data_dir))  # consistent order\n",
    "label_dict = {i: name for i, name in enumerate(label_names)}\n",
    "\n",
    "# Pick random person and image\n",
    "random_person = random.choice(label_names)\n",
    "person_path = os.path.join(data_dir, random_person)\n",
    "img_name = random.choice(os.listdir(person_path))\n",
    "img_path = os.path.join(person_path, img_name)\n",
    "\n",
    "# Load and preprocess image\n",
    "img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "img_resized = cv2.resize(img, img_size)\n",
    "img_array = img_to_array(img_resized) / 255.0\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "# Predict\n",
    "prediction = model.predict(img_array)\n",
    "predicted_class = np.argmax(prediction)\n",
    "predicted_name = label_dict[predicted_class]\n",
    "\n",
    "# Show image and prediction\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title(f'Predicted: {predicted_name}')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be62bf1-877d-485c-a03e-63cd130ca773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "\n",
    "# Set dataset path and image size\n",
    "data_dir = r'C:\\Users\\user\\Desktop\\Face Recognization\\lfw_funneled'\n",
    "img_size = (100, 100)\n",
    "\n",
    "# Build label map (same order as training)\n",
    "label_names = sorted(os.listdir(data_dir))\n",
    "label_dict = {i: name for i, name in enumerate(label_names)}\n",
    "\n",
    "# Pick a random person and image\n",
    "random_person = random.choice(label_names)\n",
    "person_path = os.path.join(data_dir, random_person)\n",
    "img_name = random.choice(os.listdir(person_path))\n",
    "img_path = os.path.join(person_path, img_name)\n",
    "\n",
    "# Load and preprocess the image\n",
    "img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "img_resized = cv2.resize(img, img_size)\n",
    "img_array = img_to_array(img_resized) / 255.0\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "# Predict\n",
    "prediction = model.predict(img_array)\n",
    "predicted_class = np.argmax(prediction)\n",
    "predicted_name = label_dict[predicted_class]\n",
    "\n",
    "# True label\n",
    "true_name = random_person\n",
    "\n",
    "# Show image and comparison\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title(f'True: {true_name}\\nPredicted: {predicted_name}')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Optional: Print result\n",
    "if true_name == predicted_name:\n",
    "    print(\"‚úÖ Prediction is correct!\")\n",
    "else:\n",
    "    print(\"‚ùå Prediction is incorrect.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129969c5-dd4a-4d89-a4f6-7a9e66e85afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "\n",
    "# Set dataset path and image size\n",
    "data_dir = r'C:\\Users\\user\\Desktop\\Face Recognization\\lfw_funneled'\n",
    "img_size = (100, 100)\n",
    "\n",
    "# Build label map (same order as training)\n",
    "label_names = sorted(os.listdir(data_dir))\n",
    "label_dict = {i: name for i, name in enumerate(label_names)}\n",
    "\n",
    "# Pick a random person and image\n",
    "random_person = random.choice(label_names)\n",
    "person_path = os.path.join(data_dir, random_person)\n",
    "img_name = random.choice(os.listdir(person_path))\n",
    "img_path = os.path.join(person_path, img_name)\n",
    "\n",
    "# Load and preprocess the image\n",
    "img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "img_resized = cv2.resize(img, img_size)\n",
    "img_array = img_to_array(img_resized) / 255.0\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "# Predict\n",
    "prediction = model.predict(img_array)\n",
    "predicted_class = np.argmax(prediction)\n",
    "predicted_name = label_dict[predicted_class]\n",
    "\n",
    "# True label\n",
    "true_name = random_person\n",
    "\n",
    "# Show image and comparison\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title(f'True: {true_name}\\nPredicted: {predicted_name}')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Optional: Print result\n",
    "if true_name == predicted_name:\n",
    "    print(\"‚úÖ Prediction is correct!\")\n",
    "else:\n",
    "    print(\"‚ùå Prediction is incorrect.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04228cf1-2a63-4433-a932-a21a05b27268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "\n",
    "# Make sure this matches the training image size\n",
    "img_size = (100, 100)\n",
    "\n",
    "# Path to your beautified dataset (adjust if needed)\n",
    "data_dir = 'LFW-Beautified-dataset/LFW-Beautified-dataset'\n",
    "\n",
    "# Get class labels (MUST use same order as during training)\n",
    "label_names = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
    "label_dict = {i: name for i, name in enumerate(label_names)}\n",
    "\n",
    "# Pick a random person and image\n",
    "random_person = random.choice(label_names)\n",
    "person_folder = os.path.join(data_dir, random_person)\n",
    "random_image = random.choice(os.listdir(person_folder))\n",
    "img_path = os.path.join(person_folder, random_image)\n",
    "\n",
    "# Load and preprocess image\n",
    "img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "img_resized = cv2.resize(img, img_size)\n",
    "img_array = img_to_array(img_resized) / 255.0\n",
    "img_array = np.expand_dims(img_array, axis=0)  # (1, 100, 100, 1)\n",
    "\n",
    "# Predict\n",
    "prediction = model.predict(img_array)\n",
    "predicted_label = np.argmax(prediction)\n",
    "predicted_name = label_dict[predicted_label]\n",
    "\n",
    "# Show result\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title(f\"Actual: {random_person} | Predicted: {predicted_name}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10d7439-8d70-4a24-9366-1c408aa39d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "\n",
    "# Make sure this matches the training image size\n",
    "img_size = (100, 100)\n",
    "\n",
    "# Path to your beautified dataset (adjust if needed)\n",
    "data_dir = 'LFW-Beautified-dataset/LFW-Beautified-dataset'\n",
    "\n",
    "# Get class labels (MUST use same order as during training)\n",
    "label_names = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
    "label_dict = {i: name for i, name in enumerate(label_names)}\n",
    "\n",
    "# Pick a random person and image\n",
    "random_person = random.choice(label_names)\n",
    "person_folder = os.path.join(data_dir, random_person)\n",
    "random_image = random.choice(os.listdir(person_folder))\n",
    "img_path = os.path.join(person_folder, random_image)\n",
    "\n",
    "# Load and preprocess image\n",
    "img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "img_resized = cv2.resize(img, img_size)\n",
    "img_array = img_to_array(img_resized) / 255.0\n",
    "img_array = np.expand_dims(img_array, axis=0)  # (1, 100, 100, 1)\n",
    "\n",
    "# Predict\n",
    "prediction = model.predict(img_array)\n",
    "predicted_label = np.argmax(prediction)\n",
    "predicted_name = label_dict[predicted_label]\n",
    "\n",
    "# Show result\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title(f\"Actual: {random_person} | Predicted: {predicted_name}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1db021-9901-4542-ba07-3c0502915e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "# Set dataset path\n",
    "data_dir = 'C:/Users/user/Desktop/Face Recognization/lfw_funneled'\n",
    "img_size = (100, 100)\n",
    "\n",
    "# Load images and labels\n",
    "X, y, label_names = [], [], []\n",
    "label_map = {}\n",
    "\n",
    "print(\"üìÇ Loading LFW funneled dataset...\")\n",
    "\n",
    "for label_index, person_name in enumerate(os.listdir(data_dir)):\n",
    "    person_path = os.path.join(data_dir, person_name)\n",
    "    if not os.path.isdir(person_path):\n",
    "        continue\n",
    "\n",
    "    label_map[label_index] = person_name\n",
    "    label_names.append(person_name)\n",
    "\n",
    "    for img_name in os.listdir(person_path):\n",
    "        img_path = os.path.join(person_path, img_name)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        img = cv2.resize(img, img_size)\n",
    "        X.append(img)\n",
    "        y.append(label_index)\n",
    "\n",
    "X = np.array(X).reshape(-1, img_size[1], img_size[0], 1) / 255.0\n",
    "y = np.array(y)\n",
    "y = to_categorical(y)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_size[1], img_size[0], 1)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(len(label_names), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "print(\"üöÄ Training model...\")\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"‚úÖ Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"lfw_funneled_model.keras\")\n",
    "print(\"üíæ Model saved as lfw_funneled_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea03152-f2bc-4130-9836-1a5ea2a23cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = 0\n",
    "for person_name in os.listdir(data_dir):\n",
    "    person_path = os.path.join(data_dir, person_name)\n",
    "    if not os.path.isdir(person_path):\n",
    "        continue\n",
    "\n",
    "    person_images = []\n",
    "    for img_name in os.listdir(person_path):\n",
    "        img_path = os.path.join(person_path, img_name)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            continue\n",
    "        img = cv2.resize(img, img_size)\n",
    "        person_images.append(img)\n",
    "\n",
    "    # Only add person if at least one valid image exists\n",
    "    if len(person_images) > 0:\n",
    "        label_map[label_index] = person_name\n",
    "        label_names.append(person_name)\n",
    "        for img in person_images:\n",
    "            X.append(img)\n",
    "            y.append(label_index)\n",
    "        label_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92c023d-7e83-4c98-ae10-670732067efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "# Define dataset path and image size\n",
    "data_dir = r'C:\\Users\\user\\Desktop\\Face Recognization\\lfw_funneled'\n",
    "img_size = (100, 100)\n",
    "\n",
    "# Initialize data containers\n",
    "X = []\n",
    "y = []\n",
    "label_map = {}\n",
    "label_names = []\n",
    "\n",
    "label_index = 0\n",
    "print(\"üì• Loading images...\")\n",
    "for person_name in os.listdir(data_dir):\n",
    "    person_path = os.path.join(data_dir, person_name)\n",
    "    if not os.path.isdir(person_path):\n",
    "        continue\n",
    "\n",
    "    person_images = []\n",
    "    for img_name in os.listdir(person_path):\n",
    "        img_path = os.path.join(person_path, img_name)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            continue\n",
    "        img = cv2.resize(img, img_size)\n",
    "        person_images.append(img)\n",
    "\n",
    "    if len(person_images) > 0:\n",
    "        label_map[label_index] = person_name\n",
    "        label_names.append(person_name)\n",
    "        for img in person_images:\n",
    "            X.append(img)\n",
    "            y.append(label_index)\n",
    "        label_index += 1\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(X).reshape(-1, img_size[0], img_size[1], 1) / 255.0\n",
    "y = to_categorical(np.array(y))\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build CNN model\n",
    "model = Sequential([\n",
    "    Input(shape=(img_size[0], img_size[1], 1)),\n",
    "    Conv2D(32, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_map), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "print(\"üöÄ Training model...\")\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"‚úÖ Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd95895-7f70-4135-85cf-18bf7625d5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "# Define dataset path and image size\n",
    "data_dir = r'C:\\Users\\user\\Desktop\\Face Recognization\\lfw_funneled'\n",
    "img_size = (100, 100)\n",
    "\n",
    "# Initialize data containers\n",
    "X = []\n",
    "y = []\n",
    "label_map = {}\n",
    "label_names = []\n",
    "\n",
    "label_index = 0\n",
    "print(\"üì• Loading images...\")\n",
    "for person_name in os.listdir(data_dir):\n",
    "    person_path = os.path.join(data_dir, person_name)\n",
    "    if not os.path.isdir(person_path):\n",
    "        continue\n",
    "\n",
    "    person_images = []\n",
    "    for img_name in os.listdir(person_path):\n",
    "        img_path = os.path.join(person_path, img_name)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            continue\n",
    "        img = cv2.resize(img, img_size)\n",
    "        person_images.append(img)\n",
    "\n",
    "    if len(person_images) > 0:\n",
    "        label_map[label_index] = person_name\n",
    "        label_names.append(person_name)\n",
    "        for img in person_images:\n",
    "            X.append(img)\n",
    "            y.append(label_index)\n",
    "        label_index += 1\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(X).reshape(-1, img_size[0], img_size[1], 1) / 255.0\n",
    "y = to_categorical(np.array(y))\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build CNN model\n",
    "model = Sequential([\n",
    "    Input(shape=(img_size[0], img_size[1], 1)),\n",
    "    Conv2D(32, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_map), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "print(\"üöÄ Training model...\")\n",
    "history = model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"‚úÖ Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd17c0e3-bf4e-46a9-9f64-26b15b9c4a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f777f1e8-19c8-4a30-8122-52c47b4e9993",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"face_lfw_funneled_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff501735-5ce8-4fa5-823f-3bfcacee5e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# ‚úÖ Load the saved face & lip model\n",
    "model = load_model(\"face_lip_model.h5\")\n",
    "\n",
    "print(\"‚úÖ face_lip_model.h5 loaded successfully!\")\n",
    "\n",
    "# Optional: summarize the model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c6dba0-4ae5-410a-a665-6eaa28e4e572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "# Check if camera opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"‚ùå Error: Could not open webcam.\")\n",
    "else:\n",
    "    print(\"‚úÖ Webcam opened successfully.\")\n",
    "\n",
    "    with mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.5) as face_detection:\n",
    "        time.sleep(1)  # Give camera some time to initialize\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Failed to grab frame\")\n",
    "                break\n",
    "\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = face_detection.process(image)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            if results.detections:\n",
    "                for detection in results.detections:\n",
    "                    mp_drawing.draw_detection(image, detection)\n",
    "\n",
    "            cv2.imshow('Real-Time Face Detection', image)\n",
    "\n",
    "            if cv2.waitKey(5) & 0xFF == 27:\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6838c664-1d1e-4d11-9b17-86e653afea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Start video capture from the correct webcam index\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "# Mediapipe setup\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# Lip landmark indices from Mediapipe's 468-point face mesh\n",
    "LIPS = list(set([\n",
    "    61, 146, 91, 181, 84, 17, 314, 405, 321, 375, 291, 308,\n",
    "    324, 318, 402, 317, 14, 87, 178, 88, 95, 185, 40, 39,\n",
    "    37, 0, 267, 269, 270, 409, 415, 310, 311, 312, 13, 82,\n",
    "    81, 42, 183, 78\n",
    "]))\n",
    "\n",
    "with mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ") as face_mesh:\n",
    "\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            print(\"‚ùå Couldn't access webcam frame.\")\n",
    "            break\n",
    "\n",
    "        # Flip and convert image to RGB\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(rgb)\n",
    "\n",
    "        # Draw lips\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                h, w, _ = frame.shape\n",
    "                for idx in LIPS:\n",
    "                    pt = face_landmarks.landmark[idx]\n",
    "                    x, y = int(pt.x * w), int(pt.y * h)\n",
    "                    cv2.circle(frame, (x, y), 2, (0, 0, 255), -1)  # red dots\n",
    "\n",
    "        cv2.imshow(\"Real-Time Lip Tracking\", frame)\n",
    "        if cv2.waitKey(5) & 0xFF == 27:  # Press ESC to exit\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591ebcaa-91e1-4186-be19-a01b96d6dfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Setup\n",
    "words = [\"HELLO\", \"THANKS\", \"BYE\"]\n",
    "samples_per_word = 5\n",
    "save_dir = \"lip_read_dataset\"\n",
    "record_duration = 3  # seconds\n",
    "\n",
    "# Create directories\n",
    "for word in words:\n",
    "    os.makedirs(os.path.join(save_dir, word), exist_ok=True)\n",
    "\n",
    "cap = cv2.VideoCapture(1)  # Use 0 or 1 based on your webcam\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"‚ùå Cannot access the webcam.\")\n",
    "else:\n",
    "    print(\"‚úÖ Webcam ready.\")\n",
    "\n",
    "    for word in words:\n",
    "        print(f\"\\nüé§ Recording for word: {word}\")\n",
    "        for i in range(samples_per_word):\n",
    "            input(f\"\\nüî¥ Press ENTER to start recording sample {i+1} of '{word}'...\")\n",
    "            print(\"‚è≥ Recording...\")\n",
    "\n",
    "            frames = []\n",
    "            start_time = time.time()\n",
    "            while time.time() - start_time < record_duration:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                cv2.imshow(\"Recording...\", frame)\n",
    "                frames.append(frame)\n",
    "\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "            # Save video\n",
    "            filename = os.path.join(save_dir, word, f\"{word}_{i+1}.avi\")\n",
    "            height, width, _ = frames[0].shape\n",
    "            out = cv2.VideoWriter(filename, cv2.VideoWriter_fourcc(*'XVID'), 20, (width, height))\n",
    "            for frame in frames:\n",
    "                out.write(frame)\n",
    "            out.release()\n",
    "            print(f\"‚úÖ Saved: {filename}\")\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"\\nüìÅ All recordings saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e8f766-a78b-437e-8723-8bd19cfd727e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import mediapipe as mp\n",
    "\n",
    "# Configuration\n",
    "words = [\"HELLO\", \"THANKS\", \"BYE\"]\n",
    "samples_per_word = 5\n",
    "frames_per_sample = 30  # ~1 second at 30fps\n",
    "output_dir = \"lip_dataset\"\n",
    "\n",
    "# Mediapipe setup\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(refine_landmarks=True)\n",
    "drawing_spec = mp.solutions.drawing_utils.DrawingSpec(thickness=1, circle_radius=1)\n",
    "\n",
    "# Lip landmark indices (inner + outer)\n",
    "lip_indices = list(set([\n",
    "    61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291,\n",
    "    146, 91, 181, 84, 17, 314, 405, 321, 375, 291\n",
    "]))\n",
    "\n",
    "# Create folders\n",
    "for word in words:\n",
    "    os.makedirs(os.path.join(output_dir, word), exist_ok=True)\n",
    "\n",
    "# Start webcam\n",
    "cap = cv2.VideoCapture(1)  # or 0, depending on your setup\n",
    "\n",
    "for word in words:\n",
    "    print(f\"Recording samples for: {word}\")\n",
    "    for sample in range(samples_per_word):\n",
    "        print(f\"Sample {sample+1}/{samples_per_word} - Say '{word}'\")\n",
    "        frames_collected = 0\n",
    "        input(\"Press Enter when ready...\")\n",
    "\n",
    "        while frames_collected < frames_per_sample:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = face_mesh.process(frame_rgb)\n",
    "\n",
    "            if results.multi_face_landmarks:\n",
    "                landmarks = results.multi_face_landmarks[0]\n",
    "                h, w, _ = frame.shape\n",
    "                lip_points = [(int(l.x * w), int(l.y * h)) for i, l in enumerate(landmarks.landmark) if i in lip_indices]\n",
    "\n",
    "                if lip_points:\n",
    "                    x_vals = [p[0] for p in lip_points]\n",
    "                    y_vals = [p[1] for p in lip_points]\n",
    "                    x1, y1 = max(min(x_vals)-10, 0), max(min(y_vals)-10, 0)\n",
    "                    x2, y2 = min(max(x_vals)+10, w), min(max(y_vals)+10, h)\n",
    "                    lip_img = frame[y1:y2, x1:x2]\n",
    "\n",
    "                    if lip_img.size != 0:\n",
    "                        save_path = os.path.join(output_dir, word, f\"{word}_{sample}_{frames_collected}.jpg\")\n",
    "                        cv2.imwrite(save_path, lip_img)\n",
    "                        frames_collected += 1\n",
    "\n",
    "            cv2.imshow('Lip Recording', frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"‚úÖ Done recording all samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf1793e-397c-47f7-954b-e5f69bc6d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Set your dataset path\n",
    "dataset_path = r\"C:\\Users\\user\\Desktop\\Face Recognization\\lip_dataset\"\n",
    "\n",
    "# Regex for .jpg files\n",
    "pattern = re.compile(r\"^(?P<word>[A-Z]+)_(?P<sample>\\d+)_(?P<frame>\\d+)\\.jpg$\")\n",
    "\n",
    "for word in os.listdir(dataset_path):\n",
    "    word_path = os.path.join(dataset_path, word)\n",
    "    if not os.path.isdir(word_path):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nüßπ Checking folder: {word}\")\n",
    "    sample_dict = {}\n",
    "\n",
    "    for file in os.listdir(word_path):\n",
    "        file_path = os.path.join(word_path, file)\n",
    "\n",
    "        # Match filename format\n",
    "        match = pattern.match(file)\n",
    "        if not match:\n",
    "            print(f\"‚ùå Deleting badly named file: {file}\")\n",
    "            os.remove(file_path)\n",
    "            continue\n",
    "\n",
    "        sample_id = int(match.group(\"sample\"))\n",
    "        frame_id = int(match.group(\"frame\"))\n",
    "\n",
    "        # Track frames per sample\n",
    "        if sample_id not in sample_dict:\n",
    "            sample_dict[sample_id] = set()\n",
    "        sample_dict[sample_id].add(frame_id)\n",
    "\n",
    "    # Check sample completeness\n",
    "    for sample_id, frames in sample_dict.items():\n",
    "        if len(frames) != 30:\n",
    "            print(f\"‚ö†Ô∏è Incomplete sample in {word} - Sample {sample_id} has {len(frames)} frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8754f2ae-c905-4cf6-bad9-b2967292cef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# üìÅ Dataset path\n",
    "DATASET_PATH = r\"C:\\Users\\user\\Desktop\\Face Recognization\\lip_dataset\"\n",
    "\n",
    "# üè∑Ô∏è Get list of word folders\n",
    "classes = sorted(os.listdir(DATASET_PATH))\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(classes)\n",
    "\n",
    "# üì¶ Containers\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# üîÅ Loop through each word folder\n",
    "for label in classes:\n",
    "    folder_path = os.path.join(DATASET_PATH, label)\n",
    "    samples = {}\n",
    "\n",
    "    # üßπ Organize all frames into samples\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".jpg\"):\n",
    "            parts = file.replace(\".jpg\", \"\").split(\"_\")\n",
    "            sample_id = int(parts[1])\n",
    "            frame_path = os.path.join(folder_path, file)\n",
    "\n",
    "            if sample_id not in samples:\n",
    "                samples[sample_id] = []\n",
    "            samples[sample_id].append(frame_path)\n",
    "\n",
    "    # üì¶ Add each sample\n",
    "    for sample_id in sorted(samples.keys()):\n",
    "        frames = sorted(samples[sample_id])  # sort frame_0 to frame_29\n",
    "        if len(frames) != 30:\n",
    "            print(f\"Skipping sample {label}_{sample_id} (has {len(frames)} frames)\")\n",
    "            continue\n",
    "\n",
    "        sequence = []\n",
    "        for frame_file in frames:\n",
    "            img = cv2.imread(frame_file, cv2.IMREAD_GRAYSCALE)\n",
    "            img = cv2.resize(img, (64, 64))  # üßä Resize to 64x64\n",
    "            img = img / 255.0  # üîò Normalize\n",
    "            sequence.append(img)\n",
    "\n",
    "        X.append(sequence)\n",
    "        y.append(label)\n",
    "\n",
    "# üß† Encode labels\n",
    "y_encoded = label_encoder.transform(y)\n",
    "\n",
    "# üßä Convert to NumPy arrays\n",
    "X = np.array(X).reshape(-1, 30, 64, 64, 1)  # (samples, time, H, W, 1)\n",
    "y = np.array(y_encoded)\n",
    "\n",
    "# üíæ Save\n",
    "np.save(\"X.npy\", X)\n",
    "np.save(\"y.npy\", y)\n",
    "\n",
    "print(\"‚úÖ Dataset saved! Shapes:\")\n",
    "print(\"X:\", X.shape)\n",
    "print(\"y:\", y.shape)\n",
    "print(\"Classes:\", label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a94e42-d16a-4aca-9797-a6ffdcb4088f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import TimeDistributed, Conv2D, MaxPooling2D, Flatten, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# üì¶ Load dataset\n",
    "X = np.load(\"X.npy\")\n",
    "y = np.load(\"y.npy\")\n",
    "\n",
    "# üè∑Ô∏è One-hot encode labels\n",
    "y_cat = to_categorical(y)\n",
    "\n",
    "# üß™ Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size=0.2, random_state=42)\n",
    "\n",
    "# üß† Build model\n",
    "model = Sequential([\n",
    "    TimeDistributed(Conv2D(32, (3,3), activation='relu'), input_shape=(30, 64, 64, 1)),\n",
    "    TimeDistributed(MaxPooling2D((2,2))),\n",
    "    TimeDistributed(Conv2D(64, (3,3), activation='relu')),\n",
    "    TimeDistributed(MaxPooling2D((2,2))),\n",
    "    TimeDistributed(Flatten()),\n",
    "    LSTM(128),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(3, activation='softmax')  # 3 classes: HELLO, THANKS, BYE\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# üèãÔ∏è Train\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=2, validation_data=(X_test, y_test))\n",
    "\n",
    "# üíæ Save model\n",
    "model.save(\"lip_reading_model.h5\")\n",
    "print(\"‚úÖ Model saved as lip_reading_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f9537d-77e3-490e-a85b-05e9c6b3bae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# üìà Accuracy\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# üìâ Loss\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49512ce-0c8a-44e0-aa84-23e16aec88dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73800966-ce7b-4ac7-862b-0bfb1991e731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from keras.models import load_model\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "# Load model and labels\n",
    "model = load_model('lip_reading_model.h5')\n",
    "labels = ['BYE', 'HELLO', 'THANKS']\n",
    "\n",
    "# Mediapipe Face Mesh setup\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, refine_landmarks=True)\n",
    "\n",
    "# Function to extract lips from frame\n",
    "def extract_lips(frame):\n",
    "    h, w, _ = frame.shape\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(rgb)\n",
    "    if results.multi_face_landmarks:\n",
    "        for landmarks in results.multi_face_landmarks:\n",
    "            lip_points = [landmarks.landmark[i] for i in list(range(61, 88))]\n",
    "            lip_coords = [(int(p.x * w), int(p.y * h)) for p in lip_points]\n",
    "            x_min = min(p[0] for p in lip_coords)\n",
    "            y_min = min(p[1] for p in lip_coords)\n",
    "            x_max = max(p[0] for p in lip_coords)\n",
    "            y_max = max(p[1] for p in lip_coords)\n",
    "            lips_img = frame[y_min:y_max, x_min:x_max]\n",
    "            if lips_img.size == 0:\n",
    "                return None\n",
    "            lips_img = cv2.resize(lips_img, (64, 64))\n",
    "            lips_img = cv2.cvtColor(lips_img, cv2.COLOR_BGR2GRAY)\n",
    "            lips_img = lips_img / 255.0\n",
    "            return lips_img\n",
    "    return None\n",
    "\n",
    "# Capture lip sequence\n",
    "def capture_sequence(sequence_length=30):\n",
    "    cap = cv2.VideoCapture(1)\n",
    "    buffer = deque(maxlen=sequence_length)\n",
    "    print(\"üìπ Get ready to say a word silently... Press 'Q' to quit early.\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        lips = extract_lips(frame)\n",
    "        if lips is not None:\n",
    "            buffer.append(lips)\n",
    "            cv2.imshow('Lip ROI', cv2.resize(lips, (128, 128)))\n",
    "        else:\n",
    "            cv2.imshow('Lip ROI', np.zeros((128, 128), dtype=np.uint8))\n",
    "\n",
    "        # Quit with Q or automatically after sequence\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q') or len(buffer) == sequence_length:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    if len(buffer) == sequence_length:\n",
    "        print(\"‚úÖ Sequence captured successfully.\")\n",
    "        return np.expand_dims(np.array(buffer), axis=0)[..., np.newaxis]\n",
    "    else:\n",
    "        print(\"‚ùå Sequence not complete.\")\n",
    "        return None\n",
    "\n",
    "# Predict\n",
    "sequence = capture_sequence()\n",
    "if sequence is not None:\n",
    "    prediction = model.predict(sequence)\n",
    "    word = labels[np.argmax(prediction)]\n",
    "    print(f\"üó£Ô∏è Predicted word: {word}\")\n",
    "else:\n",
    "    print(\"No prediction made.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedbd5b5-2c09-4fc4-bc36-809635dd44be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model('lip_reading_model.h5')\n",
    "\n",
    "# Convert to TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model\n",
    "with open('lip_reading_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71c79bda-6192-445e-98e0-378f6ad44460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\user\\AppData\\Local\\Temp\\tmplox9_pk6\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\user\\AppData\\Local\\Temp\\tmplox9_pk6\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\user\\AppData\\Local\\Temp\\tmplox9_pk6'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 30, 64, 64, 1), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 3), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1716851124432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1716851125008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1716851124624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1716851125392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1716851129616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1716851128656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1716851129808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1716851130192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1716853474896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1716853474320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1716853475856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "ename": "ConverterError",
     "evalue": "<unknown>:0: error: loc(callsite(callsite(fused[\"TensorListReserve:\", \"sequential_1/lstm_1/TensorArrayV2_1@__inference_function_3021\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_3076\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.TensorListReserve' op requires element_shape to be static during TF Lite transformation pass\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: error: loc(callsite(callsite(fused[\"TensorListReserve:\", \"sequential_1/lstm_1/TensorArrayV2_1@__inference_function_3021\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_3076\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: error: Lowering tensor list ops is failed. Please consider using Select TF ops and disabling `_experimental_lower_tensor_list_ops` flag in the TFLite converter object. For example, converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\\n converter._experimental_lower_tensor_list_ops = False\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConverterError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Convert to TFLite format\u001b[39;00m\n\u001b[0;32m      7\u001b[0m converter \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mlite\u001b[38;5;241m.\u001b[39mTFLiteConverter\u001b[38;5;241m.\u001b[39mfrom_keras_model(model)\n\u001b[1;32m----> 8\u001b[0m tflite_model \u001b[38;5;241m=\u001b[39m converter\u001b[38;5;241m.\u001b[39mconvert()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlip_reading_model.tflite\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1250\u001b[0m, in \u001b[0;36m_export_metrics.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1247\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(convert_func)\n\u001b[0;32m   1248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1249\u001b[0m   \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> 1250\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_and_export_metrics(convert_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1202\u001b[0m, in \u001b[0;36mTFLiteConverterBase._convert_and_export_metrics\u001b[1;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_conversion_params_metric()\n\u001b[0;32m   1201\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mprocess_time()\n\u001b[1;32m-> 1202\u001b[0m result \u001b[38;5;241m=\u001b[39m convert_func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1203\u001b[0m elapsed_time_ms \u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mprocess_time() \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m   1204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1768\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2.convert\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1755\u001b[0m \u001b[38;5;129m@_export_metrics\u001b[39m\n\u001b[0;32m   1756\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Converts a keras model based on instance variables.\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \n\u001b[0;32m   1759\u001b[0m \u001b[38;5;124;03m  Returns:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;124;03m      Invalid quantization parameters.\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1768\u001b[0m   saved_model_convert_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_as_saved_model()\n\u001b[0;32m   1769\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m saved_model_convert_result:\n\u001b[0;32m   1770\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_convert_result\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1749\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2._convert_as_saved_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1745\u001b[0m   graph_def, input_tensors, output_tensors \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1746\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_keras_to_saved_model(temp_dir)\n\u001b[0;32m   1747\u001b[0m   )\n\u001b[0;32m   1748\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msaved_model_dir:\n\u001b[1;32m-> 1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(TFLiteKerasModelConverterV2, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mconvert(\n\u001b[0;32m   1750\u001b[0m         graph_def, input_tensors, output_tensors\n\u001b[0;32m   1751\u001b[0m     )\n\u001b[0;32m   1752\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1753\u001b[0m   shutil\u001b[38;5;241m.\u001b[39mrmtree(temp_dir, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1487\u001b[0m, in \u001b[0;36mTFLiteConverterBaseV2.convert\u001b[1;34m(self, graph_def, input_tensors, output_tensors)\u001b[0m\n\u001b[0;32m   1480\u001b[0m   logging\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m   1481\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing new converter: If you encounter a problem \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1482\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease file a bug. You can opt-out \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1483\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby setting experimental_new_converter=False\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1484\u001b[0m   )\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;66;03m# Converts model.\u001b[39;00m\n\u001b[1;32m-> 1487\u001b[0m result \u001b[38;5;241m=\u001b[39m _convert_graphdef(\n\u001b[0;32m   1488\u001b[0m     input_data\u001b[38;5;241m=\u001b[39mgraph_def,\n\u001b[0;32m   1489\u001b[0m     input_tensors\u001b[38;5;241m=\u001b[39minput_tensors,\n\u001b[0;32m   1490\u001b[0m     output_tensors\u001b[38;5;241m=\u001b[39moutput_tensors,\n\u001b[0;32m   1491\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconverter_kwargs,\n\u001b[0;32m   1492\u001b[0m )\n\u001b[0;32m   1494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimize_tflite_model(\n\u001b[0;32m   1495\u001b[0m     result,\n\u001b[0;32m   1496\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_quant_mode,\n\u001b[0;32m   1497\u001b[0m     _build_conversion_flags(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconverter_kwargs)\u001b[38;5;241m.\u001b[39mdebug_options,\n\u001b[0;32m   1498\u001b[0m     quant_io\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_new_quantizer,\n\u001b[0;32m   1499\u001b[0m )\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\tensorflow\\lite\\python\\convert_phase.py:212\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     report_error_message(\u001b[38;5;28mstr\u001b[39m(converter_error))\n\u001b[1;32m--> 212\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m converter_error \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Re-throws the exception.\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m    214\u001b[0m   report_error_message(\u001b[38;5;28mstr\u001b[39m(error))\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\tensorflow\\lite\\python\\convert_phase.py:205\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    204\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m ConverterError \u001b[38;5;28;01mas\u001b[39;00m converter_error:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m converter_error\u001b[38;5;241m.\u001b[39merrors:\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\tensorflow\\lite\\python\\convert.py:885\u001b[0m, in \u001b[0;36mconvert_graphdef\u001b[1;34m(input_data, input_tensors, output_tensors, **kwargs)\u001b[0m\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    883\u001b[0m     model_flags\u001b[38;5;241m.\u001b[39moutput_arrays\u001b[38;5;241m.\u001b[39mappend(util\u001b[38;5;241m.\u001b[39mget_tensor_name(output_tensor))\n\u001b[1;32m--> 885\u001b[0m data \u001b[38;5;241m=\u001b[39m convert(\n\u001b[0;32m    886\u001b[0m     model_flags,\n\u001b[0;32m    887\u001b[0m     conversion_flags,\n\u001b[0;32m    888\u001b[0m     input_data\u001b[38;5;241m.\u001b[39mSerializeToString(),\n\u001b[0;32m    889\u001b[0m     debug_info_str\u001b[38;5;241m=\u001b[39mdebug_info\u001b[38;5;241m.\u001b[39mSerializeToString() \u001b[38;5;28;01mif\u001b[39;00m debug_info \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    890\u001b[0m )\n\u001b[0;32m    891\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\tensorflow\\lite\\python\\convert.py:350\u001b[0m, in \u001b[0;36mconvert\u001b[1;34m(model_flags, conversion_flags, input_data_str, debug_info_str)\u001b[0m\n\u001b[0;32m    343\u001b[0m     conversion_flags\u001b[38;5;241m.\u001b[39mguarantee_all_funcs_one_use \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert(\n\u001b[0;32m    345\u001b[0m         model_flags,\n\u001b[0;32m    346\u001b[0m         conversion_flags,\n\u001b[0;32m    347\u001b[0m         input_data_str,\n\u001b[0;32m    348\u001b[0m         debug_info_str,\n\u001b[0;32m    349\u001b[0m     )\n\u001b[1;32m--> 350\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m converter_error\n",
      "\u001b[1;31mConverterError\u001b[0m: <unknown>:0: error: loc(callsite(callsite(fused[\"TensorListReserve:\", \"sequential_1/lstm_1/TensorArrayV2_1@__inference_function_3021\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_3076\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.TensorListReserve' op requires element_shape to be static during TF Lite transformation pass\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: error: loc(callsite(callsite(fused[\"TensorListReserve:\", \"sequential_1/lstm_1/TensorArrayV2_1@__inference_function_3021\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_3076\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: error: Lowering tensor list ops is failed. Please consider using Select TF ops and disabling `_experimental_lower_tensor_list_ops` flag in the TFLite converter object. For example, converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\\n converter._experimental_lower_tensor_list_ops = False\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load your trained model\n",
    "model = tf.keras.models.load_model('lip_reading_model.h5')\n",
    "\n",
    "# Convert to TFLite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model\n",
    "with open('lip_reading_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"‚úÖ Model converted and saved as lip_reading_model.tflite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f49a3d1-f3ec-47c5-a5ac-40f3b7bcc46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\user\\AppData\\Local\\Temp\\tmpppwd_1vp\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\user\\AppData\\Local\\Temp\\tmpppwd_1vp\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\user\\AppData\\Local\\Temp\\tmpppwd_1vp'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 30, 64, 64, 1), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 3), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1716996410640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1716996416400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1716996415440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1716996417360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1716996413712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1716996419280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1716996419856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1716996419664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1716996420816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1716996418512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1716996421968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "‚úÖ Model converted with Select TF Ops and saved as lip_reading_model.tflite\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load your model\n",
    "model = tf.keras.models.load_model('lip_reading_model.h5')\n",
    "\n",
    "# Set up the converter with Select TF Ops enabled\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,  # Standard TensorFlow Lite ops\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS     # Enable TF ops like LSTM\n",
    "]\n",
    "\n",
    "# Disable lowering of TensorList ops which causes the error\n",
    "converter._experimental_lower_tensor_list_ops = False\n",
    "\n",
    "# Convert the model\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model\n",
    "with open('lip_reading_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"‚úÖ Model converted with Select TF Ops and saved as lip_reading_model.tflite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30d2acc-0410-4aaa-9517-f1a3425c24ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
